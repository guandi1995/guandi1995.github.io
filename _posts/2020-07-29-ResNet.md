---
title: "Modern Architectures in CNN - ResNet"
date: 2020-07-29
tags: [CNN]

excerpt: "Deep Learning, CNN Architectures, ResNet, Residual Blocks"
mathjax: "true"
---
<img src="{{ site.url }}{{ site.baseurl }}/images/classical_cnn/header_img.jpeg" alt="">

In this post, we mainly focus on one of the modern architectures, ResNet, that are commonly used nowadays and have much powerful abilities so as to achieve higher accuracy for the tasks such as classification and etc. Theoretically, in convolutional neural networks, the training error or accuracy is supposed to decrease and converge as the number of layers in CNN increases; however, before the introduction of ResNet, the researchers found out that as the number of convolutional layers increases, the training error does decrease but would bounce back and increase again if the number of layers increases significantly to some certain amounts. This phenomena is the so-called degradation problem. The figure below demonstrates the relationship between the training error and the number of layers in CNN in terms of "plain" networks and residual networks.



### Residual Blocks
Fortunately, the innovation of ResNet helps us cope with this intractable problem. Before diving into details, Let's take a look at how residual blocks work in residual networks. In so-called plain networks as shown below, the main path from input $$a^{[l]}$$ to $$a^{[l+2]}$$ is that  input $$a^{[l]}$$ experiences linear operations first to obtain $$z^{[l+1]}$$ and goes through the ReLU activation function afterwards to generate output $$a^{[l+1]}$$, and then applies linear operation for $$a^{[l+1]}$$ to obtain $$z^{[l+2]}$$ and goes through the ReLU activation function again to generate the final output $$a^{[l+2]}$$. In other words, 

Image

The formula described above can be summarized as:

$$z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}\\
a^{[l+1]}=g(z^{[l+1]})\\
z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]}\\
a^{[l+2]}=g(z^{[l+2]})$$

What residual block does as shown below is that instead of choosing the main path described above in plain networks, it chooses to go through a short-cut path, which is also called skip-connection that the network fast-forward/skip to right before the second ReLU activation function. Because of that, the formula for one residual block becomes as followed:

$$z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}\\
a^{[l+1]}=g(z^{[l+1]})\\
z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]}\\
a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$

The term of $$a^{[l]}$$ in the forth formula makes the network to residual network. The authors of ResNet found that using the residual blocks in network allows to train much deeper networks.

### Residual Network
The way how residual networks work is that by stacking each residual block shown above enables us to train deeper networks. In other words, we can skip the training of few layers using skip-connections or residual blocks.
